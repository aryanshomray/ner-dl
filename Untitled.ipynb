{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from preprocessing import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\Machine Learning\\Natural Lannguage Processing\\ner-dl\\data\\CoNLL-2003\n"
     ]
    }
   ],
   "source": [
    "%cd CoNLL-2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is 4840-F457\n",
      "\n",
      " Directory of D:\\Projects\\Machine Learning\\Natural Lannguage Processing\\ner-dl\\data\\CoNLL-2003\n",
      "\n",
      "06/28/2017  08:35 AM    <DIR>          .\n",
      "06/28/2017  08:35 AM    <DIR>          ..\n",
      "06/28/2017  08:35 AM           826,992 eng.testa\n",
      "06/28/2017  08:35 AM           832,337 eng.testa.openNLP\n",
      "06/28/2017  08:35 AM           748,074 eng.testb\n",
      "06/28/2017  08:35 AM           753,276 eng.testb.openNLP\n",
      "06/28/2017  08:35 AM         3,281,508 eng.train\n",
      "06/28/2017  08:35 AM         3,303,660 eng.train.openNLP\n",
      "               6 File(s)      9,745,847 bytes\n",
      "               2 Dir(s)  309,394,382,848 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data/CoNLL-2003/eng.testa',header=None,delimiter=' ')\n",
    "df.columns=['word','POS','chunk tag','named entity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "      <th>chunk tag</th>\n",
       "      <th>named entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRICKET</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LEICESTERSHIRE</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TAKE</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OVER</td>\n",
       "      <td>IN</td>\n",
       "      <td>I-PP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AT</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TOP</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AFTER</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>INNINGS</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>VICTORY</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LONDON</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>I-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1996-08-30</td>\n",
       "      <td>CD</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>West</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>I-MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Indian</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>I-MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>all-rounder</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Phil</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>I-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Simmons</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>I-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>took</td>\n",
       "      <td>VBD</td>\n",
       "      <td>I-VP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>four</td>\n",
       "      <td>CD</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>for</td>\n",
       "      <td>IN</td>\n",
       "      <td>I-PP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>38</td>\n",
       "      <td>CD</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "      <td>I-PP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Friday</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>as</td>\n",
       "      <td>IN</td>\n",
       "      <td>I-PP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Leicestershire</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>beat</td>\n",
       "      <td>VBD</td>\n",
       "      <td>I-VP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Somerset</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>by</td>\n",
       "      <td>IN</td>\n",
       "      <td>I-PP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>an</td>\n",
       "      <td>DT</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49598</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49599</th>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49600</th>\n",
       "      <td>DSE</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49601</th>\n",
       "      <td>all</td>\n",
       "      <td>DT</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49602</th>\n",
       "      <td>share</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49603</th>\n",
       "      <td>price</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49604</th>\n",
       "      <td>index</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49605</th>\n",
       "      <td>closed</td>\n",
       "      <td>VBD</td>\n",
       "      <td>I-VP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49606</th>\n",
       "      <td>2.73</td>\n",
       "      <td>CD</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49607</th>\n",
       "      <td>points</td>\n",
       "      <td>NNS</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49608</th>\n",
       "      <td>or</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49609</th>\n",
       "      <td>0.22</td>\n",
       "      <td>CD</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49610</th>\n",
       "      <td>percent</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49611</th>\n",
       "      <td>up</td>\n",
       "      <td>IN</td>\n",
       "      <td>I-PP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49612</th>\n",
       "      <td>at</td>\n",
       "      <td>IN</td>\n",
       "      <td>B-PP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49613</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49614</th>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "      <td>I-PP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49615</th>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49616</th>\n",
       "      <td>turnover</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49617</th>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>I-PP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49618</th>\n",
       "      <td>133.7</td>\n",
       "      <td>CD</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49619</th>\n",
       "      <td>million</td>\n",
       "      <td>CD</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49620</th>\n",
       "      <td>taka</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49621</th>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "      <td>I-PP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49622</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49623</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49624</th>\n",
       "      <td>--</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49625</th>\n",
       "      <td>Dhaka</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49626</th>\n",
       "      <td>Newsroom</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49627</th>\n",
       "      <td>880-2-506363</td>\n",
       "      <td>CD</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49628 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word  POS chunk tag named entity\n",
       "0             CRICKET  NNP      I-NP            O\n",
       "1                   -    :         O            O\n",
       "2      LEICESTERSHIRE  NNP      I-NP        I-ORG\n",
       "3                TAKE  NNP      I-NP            O\n",
       "4                OVER   IN      I-PP            O\n",
       "5                  AT  NNP      I-NP            O\n",
       "6                 TOP  NNP      I-NP            O\n",
       "7               AFTER  NNP      I-NP            O\n",
       "8             INNINGS  NNP      I-NP            O\n",
       "9             VICTORY   NN      I-NP            O\n",
       "10                  .    .         O            O\n",
       "11             LONDON  NNP      I-NP        I-LOC\n",
       "12         1996-08-30   CD      I-NP            O\n",
       "13               West  NNP      I-NP       I-MISC\n",
       "14             Indian  NNP      I-NP       I-MISC\n",
       "15        all-rounder   NN      I-NP            O\n",
       "16               Phil  NNP      I-NP        I-PER\n",
       "17            Simmons  NNP      I-NP        I-PER\n",
       "18               took  VBD      I-VP            O\n",
       "19               four   CD      I-NP            O\n",
       "20                for   IN      I-PP            O\n",
       "21                 38   CD      I-NP            O\n",
       "22                 on   IN      I-PP            O\n",
       "23             Friday  NNP      I-NP            O\n",
       "24                 as   IN      I-PP            O\n",
       "25     Leicestershire  NNP      I-NP        I-ORG\n",
       "26               beat  VBD      I-VP            O\n",
       "27           Somerset  NNP      I-NP        I-ORG\n",
       "28                 by   IN      I-PP            O\n",
       "29                 an   DT      I-NP            O\n",
       "...               ...  ...       ...          ...\n",
       "49598               .    .         O            O\n",
       "49599             The   DT      I-NP            O\n",
       "49600             DSE   NN      I-NP        I-ORG\n",
       "49601             all   DT      B-NP            O\n",
       "49602           share   NN      I-NP            O\n",
       "49603           price   NN      I-NP            O\n",
       "49604           index   NN      I-NP            O\n",
       "49605          closed  VBD      I-VP            O\n",
       "49606            2.73   CD      I-NP            O\n",
       "49607          points  NNS      I-NP            O\n",
       "49608              or   CC         O            O\n",
       "49609            0.22   CD      I-NP            O\n",
       "49610         percent   NN      I-NP            O\n",
       "49611              up   IN      I-PP            O\n",
       "49612              at   IN      B-PP            O\n",
       "49613               1  NaN       NaN          NaN\n",
       "49614              on   IN      I-PP            O\n",
       "49615               a   DT      I-NP            O\n",
       "49616        turnover   NN      I-NP            O\n",
       "49617              of   IN      I-PP            O\n",
       "49618           133.7   CD      I-NP            O\n",
       "49619         million   CD      I-NP            O\n",
       "49620            taka   NN      I-NP            O\n",
       "49621              on   IN      I-PP            O\n",
       "49622        Thursday  NNP      I-NP            O\n",
       "49623               .    .         O            O\n",
       "49624              --    :         O            O\n",
       "49625           Dhaka  NNP      I-NP        I-ORG\n",
       "49626        Newsroom  NNP      I-NP        I-ORG\n",
       "49627    880-2-506363   CD      I-NP            O\n",
       "\n",
       "[49628 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=process('data/CoNLL-2003/eng.testa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-b17624c60706>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/CoNLL-2003/eng.testa'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/CoNLL-2003/eng.testa'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Projects\\Machine Learning\\Natural Lannguage Processing\\ner-dl\\preprocessing.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0ml\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'data'"
     ]
    }
   ],
   "source": [
    "process('data/CoNLL-2003/eng.testa').get_data('data/CoNLL-2003/eng.testa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word            643\n",
       "POS             999\n",
       "chunk tag       359\n",
       "named entity    359\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Successful\n",
      "Fitting Successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eve', 'geo', 'org', 'gpe', 'tim', 'art', 'O', 'per', 'nat'}\n",
      "9\n",
      "1000 done\n",
      "2000 done\n",
      "3000 done\n",
      "4000 done\n",
      "5000 done\n",
      "6000 done\n",
      "7000 done\n",
      "8000 done\n",
      "9000 done\n",
      "10000 done\n",
      "11000 done\n",
      "12000 done\n",
      "13000 done\n",
      "14000 done\n",
      "15000 done\n",
      "16000 done\n",
      "17000 done\n",
      "18000 done\n",
      "19000 done\n",
      "20000 done\n",
      "21000 done\n",
      "22000 done\n",
      "23000 done\n",
      "24000 done\n",
      "25000 done\n",
      "26000 done\n",
      "vec done\n",
      "Padding done\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import Process\n",
    "from model import MyModel\n",
    "print('Import Successful')\n",
    "import spacy\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "process = Process()\n",
    "process.fit_data('data/ner_dataset.csv')\n",
    "print('Fitting Successful')\n",
    "print((process.corr_tags))\n",
    "print((process.tags))\n",
    "print(len(process.tags))\n",
    "\n",
    "\n",
    "\n",
    "process.vector_gen()\n",
    "print('vec done')\n",
    "process.padding()\n",
    "print('Padding done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.train[79][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1c207661a88c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprocess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Projects\\Machine Learning\\ner-dl\\preprocessing.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrained_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'trained_sentences.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corr_tags.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'words.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    445\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[1;32m--> 447\u001b[1;33m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[0;32m    448\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m             \u001b[1;31m# Try a pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;31m# The array contained Python objects. We need to unpickle the data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m             raise ValueError(\"Object arrays cannot be loaded when \"\n\u001b[0m\u001b[0;32m    697\u001b[0m                              \"allow_pickle=False\")\n\u001b[0;32m    698\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "process = Process()\n",
    "process.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [-0.21545067, -1.2618277 , -4.7554884 , ..., -0.72230554,\n",
       "         -2.9345422 ,  2.7720463 ],\n",
       "        [-0.81319916, -2.3443522 , -1.6110243 , ..., -1.227633  ,\n",
       "         -1.2558229 ,  0.922853  ],\n",
       "        [ 5.320428  , -1.2954875 ,  0.23499489, ...,  0.37910768,\n",
       "         -0.6260561 ,  4.9929333 ]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 4.3809133 ,  4.4260874 , -1.026637  , ..., -2.2080896 ,\n",
       "          1.6658663 ,  3.8610172 ],\n",
       "        [-1.5532537 , -2.9138222 , -4.3196177 , ...,  1.6400751 ,\n",
       "         -0.30946726, -1.2316004 ],\n",
       "        [ 2.0577307 ,  1.5304792 , -0.0384464 , ...,  0.75465626,\n",
       "         -3.096853  ,  3.017017  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 4.558131  , -2.6288133 ,  2.4824936 , ..., -0.5926035 ,\n",
       "         -1.302095  ,  2.8808825 ],\n",
       "        [ 6.990059  , -2.3437314 ,  1.1826832 , ..., -1.8691115 ,\n",
       "         -2.0958245 ,  5.8906174 ],\n",
       "        [ 1.8095607 ,  0.61205906, -1.7251825 , ..., -3.8472247 ,\n",
       "         -1.2137922 ,  3.6305585 ]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [-0.74493587, -2.555602  , -4.514429  , ...,  2.0510867 ,\n",
       "         -0.18842638, -2.7863796 ],\n",
       "        [-2.0614207 , -0.24753371, -3.8887534 , ..., -2.12702   ,\n",
       "          1.3841062 , -0.3525752 ],\n",
       "        [ 6.932122  , -3.164534  ,  3.9667163 , ..., -1.8892994 ,\n",
       "         -1.6985257 ,  3.107092  ]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [-1.4933608 ,  2.3838239 ,  0.6051084 , ..., -2.3060632 ,\n",
       "         -1.4177802 ,  2.8503184 ],\n",
       "        [-1.1362579 , -2.0372524 , -4.337152  , ...,  1.6064614 ,\n",
       "         -2.3721848 ,  0.58292425],\n",
       "        [ 4.1173587 , -1.5082626 , -0.33446023, ..., -2.2049963 ,\n",
       "         -1.9251487 ,  4.4767065 ]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(process.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(process.train,process.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0703 17:26:35.490972 23944 deprecation.py:506] From d:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0703 17:26:35.494961 23944 deprecation.py:506] From d:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0703 17:26:35.495965 23944 deprecation.py:506] From d:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0703 17:26:35.496958 23944 deprecation.py:506] From d:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model.model_compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8aaea4315a74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "model.train(process.train,process.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-c72315b99576>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0703 17:26:42.396835 23944 deprecation.py:323] From d:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13184/47959 [=======>......................] - ETA: 10:11 - loss: 0.0763 - acc: 0.11 - ETA: 5:53 - loss: 0.0545 - acc: 0.5117 - ETA: 4:24 - loss: 0.0462 - acc: 0.646 - ETA: 3:40 - loss: 0.0419 - acc: 0.712 - ETA: 3:13 - loss: 0.0393 - acc: 0.751 - ETA: 2:54 - loss: 0.0375 - acc: 0.772 - ETA: 2:40 - loss: 0.0356 - acc: 0.762 - ETA: 2:29 - loss: 0.0340 - acc: 0.737 - ETA: 2:20 - loss: 0.0333 - acc: 0.717 - ETA: 2:14 - loss: 0.0329 - acc: 0.699 - ETA: 2:08 - loss: 0.0319 - acc: 0.683 - ETA: 2:04 - loss: 0.0320 - acc: 0.668 - ETA: 2:00 - loss: 0.0317 - acc: 0.661 - ETA: 1:56 - loss: 0.0314 - acc: 0.655 - ETA: 1:53 - loss: 0.0314 - acc: 0.644 - ETA: 1:50 - loss: 0.0310 - acc: 0.640 - ETA: 1:48 - loss: 0.0308 - acc: 0.634 - ETA: 1:45 - loss: 0.0307 - acc: 0.629 - ETA: 1:43 - loss: 0.0304 - acc: 0.625 - ETA: 1:41 - loss: 0.0302 - acc: 0.623 - ETA: 1:40 - loss: 0.0299 - acc: 0.620 - ETA: 1:39 - loss: 0.0297 - acc: 0.619 - ETA: 1:37 - loss: 0.0296 - acc: 0.617 - ETA: 1:36 - loss: 0.0294 - acc: 0.615 - ETA: 1:35 - loss: 0.0293 - acc: 0.613 - ETA: 1:34 - loss: 0.0292 - acc: 0.611 - ETA: 1:33 - loss: 0.0291 - acc: 0.609 - ETA: 1:32 - loss: 0.0289 - acc: 0.607 - ETA: 1:31 - loss: 0.0288 - acc: 0.604 - ETA: 1:30 - loss: 0.0287 - acc: 0.603 - ETA: 1:29 - loss: 0.0285 - acc: 0.604 - ETA: 1:28 - loss: 0.0283 - acc: 0.601 - ETA: 1:28 - loss: 0.0282 - acc: 0.601 - ETA: 1:27 - loss: 0.0280 - acc: 0.602 - ETA: 1:26 - loss: 0.0280 - acc: 0.602 - ETA: 1:26 - loss: 0.0279 - acc: 0.601 - ETA: 1:25 - loss: 0.0278 - acc: 0.601 - ETA: 1:25 - loss: 0.0277 - acc: 0.602 - ETA: 1:24 - loss: 0.0276 - acc: 0.603 - ETA: 1:23 - loss: 0.0276 - acc: 0.604 - ETA: 1:23 - loss: 0.0275 - acc: 0.603 - ETA: 1:22 - loss: 0.0276 - acc: 0.604 - ETA: 1:22 - loss: 0.0276 - acc: 0.604 - ETA: 1:22 - loss: 0.0275 - acc: 0.604 - ETA: 1:21 - loss: 0.0274 - acc: 0.603 - ETA: 1:21 - loss: 0.0273 - acc: 0.603 - ETA: 1:20 - loss: 0.0273 - acc: 0.603 - ETA: 1:20 - loss: 0.0272 - acc: 0.603 - ETA: 1:20 - loss: 0.0272 - acc: 0.602 - ETA: 1:19 - loss: 0.0272 - acc: 0.601 - ETA: 1:19 - loss: 0.0271 - acc: 0.601 - ETA: 1:18 - loss: 0.0270 - acc: 0.600 - ETA: 1:18 - loss: 0.0269 - acc: 0.599 - ETA: 1:18 - loss: 0.0269 - acc: 0.598 - ETA: 1:18 - loss: 0.0270 - acc: 0.597 - ETA: 1:17 - loss: 0.0270 - acc: 0.596 - ETA: 1:17 - loss: 0.0270 - acc: 0.596 - ETA: 1:16 - loss: 0.0269 - acc: 0.595 - ETA: 1:16 - loss: 0.0269 - acc: 0.594 - ETA: 1:16 - loss: 0.0269 - acc: 0.593 - ETA: 1:16 - loss: 0.0269 - acc: 0.593 - ETA: 1:15 - loss: 0.0268 - acc: 0.593 - ETA: 1:15 - loss: 0.0268 - acc: 0.592 - ETA: 1:15 - loss: 0.0268 - acc: 0.592 - ETA: 1:15 - loss: 0.0267 - acc: 0.592 - ETA: 1:14 - loss: 0.0267 - acc: 0.592 - ETA: 1:14 - loss: 0.0267 - acc: 0.592 - ETA: 1:14 - loss: 0.0266 - acc: 0.592 - ETA: 1:13 - loss: 0.0266 - acc: 0.592 - ETA: 1:13 - loss: 0.0266 - acc: 0.593 - ETA: 1:13 - loss: 0.0266 - acc: 0.593 - ETA: 1:13 - loss: 0.0265 - acc: 0.593 - ETA: 1:13 - loss: 0.0265 - acc: 0.593 - ETA: 1:12 - loss: 0.0265 - acc: 0.593 - ETA: 1:12 - loss: 0.0264 - acc: 0.593 - ETA: 1:12 - loss: 0.0263 - acc: 0.593 - ETA: 1:12 - loss: 0.0263 - acc: 0.593 - ETA: 1:11 - loss: 0.0262 - acc: 0.593 - ETA: 1:11 - loss: 0.0262 - acc: 0.592 - ETA: 1:11 - loss: 0.0262 - acc: 0.592 - ETA: 1:11 - loss: 0.0261 - acc: 0.592 - ETA: 1:11 - loss: 0.0261 - acc: 0.592 - ETA: 1:11 - loss: 0.0260 - acc: 0.593 - ETA: 1:10 - loss: 0.0260 - acc: 0.592 - ETA: 1:10 - loss: 0.0259 - acc: 0.592 - ETA: 1:10 - loss: 0.0259 - acc: 0.592 - ETA: 1:10 - loss: 0.0258 - acc: 0.591 - ETA: 1:10 - loss: 0.0257 - acc: 0.590 - ETA: 1:09 - loss: 0.0257 - acc: 0.590 - ETA: 1:09 - loss: 0.0257 - acc: 0.589 - ETA: 1:09 - loss: 0.0256 - acc: 0.589 - ETA: 1:09 - loss: 0.0256 - acc: 0.588 - ETA: 1:09 - loss: 0.0255 - acc: 0.588 - ETA: 1:08 - loss: 0.0254 - acc: 0.588 - ETA: 1:08 - loss: 0.0254 - acc: 0.588 - ETA: 1:08 - loss: 0.0253 - acc: 0.588 - ETA: 1:08 - loss: 0.0252 - acc: 0.588 - ETA: 1:08 - loss: 0.0252 - acc: 0.587 - ETA: 1:08 - loss: 0.0251 - acc: 0.588 - ETA: 1:07 - loss: 0.0250 - acc: 0.588 - ETA: 1:07 - loss: 0.0249 - acc: 0.587 - ETA: 1:07 - loss: 0.0249 - acc: 0.588 - ETA: 1:07 - loss: 0.0248 - acc: 0.587 - ETA: 1:07 - loss: 0.0248 - acc: 0.587 - ETA: 1:07 - loss: 0.0247 - acc: 0.587 - ETA: 1:07 - loss: 0.0246 - acc: 0.587 - ETA: 1:06 - loss: 0.0246 - acc: 0.586 - ETA: 1:06 - loss: 0.0245 - acc: 0.586 - ETA: 1:06 - loss: 0.0244 - acc: 0.586 - ETA: 1:06 - loss: 0.0243 - acc: 0.586 - ETA: 1:06 - loss: 0.0243 - acc: 0.586 - ETA: 1:06 - loss: 0.0242 - acc: 0.586 - ETA: 1:06 - loss: 0.0241 - acc: 0.586 - ETA: 1:05 - loss: 0.0241 - acc: 0.586 - ETA: 1:05 - loss: 0.0240 - acc: 0.587 - ETA: 1:05 - loss: 0.0240 - acc: 0.586 - ETA: 1:05 - loss: 0.0239 - acc: 0.586 - ETA: 1:05 - loss: 0.0239 - acc: 0.586 - ETA: 1:05 - loss: 0.0238 - acc: 0.586 - ETA: 1:05 - loss: 0.0237 - acc: 0.586 - ETA: 1:04 - loss: 0.0237 - acc: 0.586 - ETA: 1:04 - loss: 0.0236 - acc: 0.586 - ETA: 1:04 - loss: 0.0235 - acc: 0.586 - ETA: 1:04 - loss: 0.0235 - acc: 0.586 - ETA: 1:04 - loss: 0.0234 - acc: 0.586 - ETA: 1:04 - loss: 0.0234 - acc: 0.586 - ETA: 1:04 - loss: 0.0233 - acc: 0.586 - ETA: 1:03 - loss: 0.0233 - acc: 0.586 - ETA: 1:03 - loss: 0.0232 - acc: 0.585 - ETA: 1:03 - loss: 0.0231 - acc: 0.585 - ETA: 1:03 - loss: 0.0231 - acc: 0.585 - ETA: 1:03 - loss: 0.0230 - acc: 0.585 - ETA: 1:03 - loss: 0.0230 - acc: 0.585 - ETA: 1:03 - loss: 0.0229 - acc: 0.584 - ETA: 1:02 - loss: 0.0229 - acc: 0.585 - ETA: 1:02 - loss: 0.0228 - acc: 0.585 - ETA: 1:02 - loss: 0.0227 - acc: 0.585 - ETA: 1:02 - loss: 0.0227 - acc: 0.584 - ETA: 1:02 - loss: 0.0226 - acc: 0.584 - ETA: 1:02 - loss: 0.0226 - acc: 0.584 - ETA: 1:02 - loss: 0.0225 - acc: 0.584 - ETA: 1:02 - loss: 0.0225 - acc: 0.584 - ETA: 1:01 - loss: 0.0224 - acc: 0.584 - ETA: 1:01 - loss: 0.0224 - acc: 0.584 - ETA: 1:01 - loss: 0.0223 - acc: 0.584 - ETA: 1:01 - loss: 0.0223 - acc: 0.584 - ETA: 1:01 - loss: 0.0222 - acc: 0.584 - ETA: 1:01 - loss: 0.0222 - acc: 0.584 - ETA: 1:01 - loss: 0.0221 - acc: 0.584 - ETA: 1:01 - loss: 0.0221 - acc: 0.584 - ETA: 1:01 - loss: 0.0221 - acc: 0.584 - ETA: 1:00 - loss: 0.0220 - acc: 0.584 - ETA: 1:00 - loss: 0.0220 - acc: 0.584 - ETA: 1:00 - loss: 0.0219 - acc: 0.584 - ETA: 1:00 - loss: 0.0219 - acc: 0.584 - ETA: 1:00 - loss: 0.0219 - acc: 0.584 - ETA: 1:00 - loss: 0.0218 - acc: 0.584 - ETA: 1:00 - loss: 0.0218 - acc: 0.584 - ETA: 1:00 - loss: 0.0217 - acc: 0.584 - ETA: 59s - loss: 0.0217 - acc: 0.584 - ETA: 59s - loss: 0.0217 - acc: 0.58 - ETA: 59s - loss: 0.0216 - acc: 0.58 - ETA: 59s - loss: 0.0216 - acc: 0.58 - ETA: 59s - loss: 0.0216 - acc: 0.58 - ETA: 59s - loss: 0.0215 - acc: 0.58 - ETA: 59s - loss: 0.0215 - acc: 0.58 - ETA: 59s - loss: 0.0214 - acc: 0.58 - ETA: 58s - loss: 0.0214 - acc: 0.58 - ETA: 58s - loss: 0.0213 - acc: 0.58 - ETA: 58s - loss: 0.0213 - acc: 0.58 - ETA: 58s - loss: 0.0213 - acc: 0.58 - ETA: 58s - loss: 0.0212 - acc: 0.58 - ETA: 58s - loss: 0.0212 - acc: 0.58 - ETA: 58s - loss: 0.0212 - acc: 0.58 - ETA: 58s - loss: 0.0211 - acc: 0.58 - ETA: 57s - loss: 0.0211 - acc: 0.58 - ETA: 57s - loss: 0.0211 - acc: 0.58 - ETA: 57s - loss: 0.0210 - acc: 0.58 - ETA: 57s - loss: 0.0210 - acc: 0.58 - ETA: 57s - loss: 0.0210 - acc: 0.58 - ETA: 57s - loss: 0.0209 - acc: 0.58 - ETA: 57s - loss: 0.0209 - acc: 0.58 - ETA: 57s - loss: 0.0208 - acc: 0.58 - ETA: 56s - loss: 0.0208 - acc: 0.58 - ETA: 56s - loss: 0.0208 - acc: 0.58 - ETA: 56s - loss: 0.0207 - acc: 0.58 - ETA: 56s - loss: 0.0207 - acc: 0.58 - ETA: 56s - loss: 0.0207 - acc: 0.58 - ETA: 56s - loss: 0.0206 - acc: 0.58 - ETA: 56s - loss: 0.0206 - acc: 0.58 - ETA: 56s - loss: 0.0206 - acc: 0.58 - ETA: 56s - loss: 0.0205 - acc: 0.58 - ETA: 55s - loss: 0.0205 - acc: 0.58 - ETA: 55s - loss: 0.0205 - acc: 0.58 - ETA: 55s - loss: 0.0204 - acc: 0.58 - ETA: 55s - loss: 0.0204 - acc: 0.58 - ETA: 55s - loss: 0.0204 - acc: 0.58 - ETA: 55s - loss: 0.0203 - acc: 0.58 - ETA: 55s - loss: 0.0203 - acc: 0.58 - ETA: 55s - loss: 0.0203 - acc: 0.58 - ETA: 55s - loss: 0.0202 - acc: 0.58 - ETA: 54s - loss: 0.0202 - acc: 0.58 - ETA: 54s - loss: 0.0202 - acc: 0.58 - ETA: 54s - loss: 0.0202 - acc: 0.58 - ETA: 54s - loss: 0.0201 - acc: 0.58 - ETA: 54s - loss: 0.0201 - acc: 0.5885"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26944/47959 [===============>..............] - ETA: 54s - loss: 0.0201 - acc: 0.58 - ETA: 54s - loss: 0.0200 - acc: 0.58 - ETA: 54s - loss: 0.0200 - acc: 0.58 - ETA: 54s - loss: 0.0200 - acc: 0.58 - ETA: 54s - loss: 0.0199 - acc: 0.59 - ETA: 53s - loss: 0.0199 - acc: 0.59 - ETA: 53s - loss: 0.0199 - acc: 0.59 - ETA: 53s - loss: 0.0199 - acc: 0.59 - ETA: 53s - loss: 0.0198 - acc: 0.59 - ETA: 53s - loss: 0.0198 - acc: 0.59 - ETA: 53s - loss: 0.0198 - acc: 0.59 - ETA: 53s - loss: 0.0198 - acc: 0.59 - ETA: 53s - loss: 0.0197 - acc: 0.59 - ETA: 53s - loss: 0.0197 - acc: 0.59 - ETA: 53s - loss: 0.0197 - acc: 0.59 - ETA: 53s - loss: 0.0197 - acc: 0.59 - ETA: 53s - loss: 0.0196 - acc: 0.59 - ETA: 53s - loss: 0.0196 - acc: 0.59 - ETA: 52s - loss: 0.0196 - acc: 0.59 - ETA: 52s - loss: 0.0195 - acc: 0.59 - ETA: 52s - loss: 0.0195 - acc: 0.59 - ETA: 52s - loss: 0.0195 - acc: 0.59 - ETA: 52s - loss: 0.0195 - acc: 0.59 - ETA: 52s - loss: 0.0195 - acc: 0.59 - ETA: 52s - loss: 0.0194 - acc: 0.59 - ETA: 52s - loss: 0.0194 - acc: 0.59 - ETA: 52s - loss: 0.0194 - acc: 0.59 - ETA: 52s - loss: 0.0194 - acc: 0.59 - ETA: 52s - loss: 0.0193 - acc: 0.59 - ETA: 52s - loss: 0.0193 - acc: 0.59 - ETA: 51s - loss: 0.0193 - acc: 0.59 - ETA: 51s - loss: 0.0193 - acc: 0.59 - ETA: 51s - loss: 0.0192 - acc: 0.59 - ETA: 51s - loss: 0.0192 - acc: 0.59 - ETA: 51s - loss: 0.0192 - acc: 0.59 - ETA: 51s - loss: 0.0192 - acc: 0.59 - ETA: 51s - loss: 0.0191 - acc: 0.59 - ETA: 51s - loss: 0.0191 - acc: 0.59 - ETA: 51s - loss: 0.0191 - acc: 0.59 - ETA: 51s - loss: 0.0191 - acc: 0.59 - ETA: 51s - loss: 0.0191 - acc: 0.59 - ETA: 50s - loss: 0.0190 - acc: 0.59 - ETA: 50s - loss: 0.0190 - acc: 0.59 - ETA: 50s - loss: 0.0190 - acc: 0.59 - ETA: 50s - loss: 0.0190 - acc: 0.59 - ETA: 50s - loss: 0.0189 - acc: 0.59 - ETA: 50s - loss: 0.0189 - acc: 0.59 - ETA: 50s - loss: 0.0189 - acc: 0.59 - ETA: 50s - loss: 0.0189 - acc: 0.59 - ETA: 50s - loss: 0.0189 - acc: 0.59 - ETA: 50s - loss: 0.0188 - acc: 0.59 - ETA: 49s - loss: 0.0188 - acc: 0.59 - ETA: 49s - loss: 0.0188 - acc: 0.59 - ETA: 49s - loss: 0.0188 - acc: 0.59 - ETA: 49s - loss: 0.0187 - acc: 0.59 - ETA: 49s - loss: 0.0187 - acc: 0.59 - ETA: 49s - loss: 0.0187 - acc: 0.59 - ETA: 49s - loss: 0.0187 - acc: 0.59 - ETA: 49s - loss: 0.0187 - acc: 0.59 - ETA: 49s - loss: 0.0186 - acc: 0.59 - ETA: 49s - loss: 0.0186 - acc: 0.59 - ETA: 49s - loss: 0.0186 - acc: 0.59 - ETA: 48s - loss: 0.0186 - acc: 0.59 - ETA: 48s - loss: 0.0186 - acc: 0.59 - ETA: 48s - loss: 0.0185 - acc: 0.59 - ETA: 48s - loss: 0.0185 - acc: 0.59 - ETA: 48s - loss: 0.0185 - acc: 0.59 - ETA: 48s - loss: 0.0185 - acc: 0.59 - ETA: 48s - loss: 0.0185 - acc: 0.59 - ETA: 48s - loss: 0.0184 - acc: 0.59 - ETA: 48s - loss: 0.0184 - acc: 0.59 - ETA: 48s - loss: 0.0184 - acc: 0.59 - ETA: 48s - loss: 0.0184 - acc: 0.59 - ETA: 47s - loss: 0.0184 - acc: 0.59 - ETA: 47s - loss: 0.0183 - acc: 0.59 - ETA: 47s - loss: 0.0183 - acc: 0.59 - ETA: 47s - loss: 0.0183 - acc: 0.59 - ETA: 47s - loss: 0.0183 - acc: 0.59 - ETA: 47s - loss: 0.0183 - acc: 0.59 - ETA: 47s - loss: 0.0183 - acc: 0.59 - ETA: 47s - loss: 0.0182 - acc: 0.59 - ETA: 47s - loss: 0.0182 - acc: 0.59 - ETA: 47s - loss: 0.0182 - acc: 0.59 - ETA: 47s - loss: 0.0182 - acc: 0.59 - ETA: 46s - loss: 0.0182 - acc: 0.59 - ETA: 46s - loss: 0.0181 - acc: 0.59 - ETA: 46s - loss: 0.0181 - acc: 0.59 - ETA: 46s - loss: 0.0181 - acc: 0.59 - ETA: 46s - loss: 0.0181 - acc: 0.59 - ETA: 46s - loss: 0.0181 - acc: 0.59 - ETA: 46s - loss: 0.0181 - acc: 0.59 - ETA: 46s - loss: 0.0181 - acc: 0.59 - ETA: 46s - loss: 0.0180 - acc: 0.59 - ETA: 46s - loss: 0.0180 - acc: 0.59 - ETA: 45s - loss: 0.0180 - acc: 0.59 - ETA: 45s - loss: 0.0180 - acc: 0.59 - ETA: 45s - loss: 0.0180 - acc: 0.59 - ETA: 45s - loss: 0.0180 - acc: 0.59 - ETA: 45s - loss: 0.0179 - acc: 0.59 - ETA: 45s - loss: 0.0179 - acc: 0.59 - ETA: 45s - loss: 0.0179 - acc: 0.59 - ETA: 45s - loss: 0.0179 - acc: 0.59 - ETA: 45s - loss: 0.0179 - acc: 0.59 - ETA: 45s - loss: 0.0179 - acc: 0.59 - ETA: 44s - loss: 0.0178 - acc: 0.59 - ETA: 44s - loss: 0.0178 - acc: 0.59 - ETA: 44s - loss: 0.0178 - acc: 0.59 - ETA: 44s - loss: 0.0178 - acc: 0.59 - ETA: 44s - loss: 0.0178 - acc: 0.59 - ETA: 44s - loss: 0.0178 - acc: 0.59 - ETA: 44s - loss: 0.0177 - acc: 0.59 - ETA: 44s - loss: 0.0177 - acc: 0.59 - ETA: 44s - loss: 0.0177 - acc: 0.59 - ETA: 44s - loss: 0.0177 - acc: 0.59 - ETA: 44s - loss: 0.0177 - acc: 0.59 - ETA: 43s - loss: 0.0177 - acc: 0.59 - ETA: 43s - loss: 0.0177 - acc: 0.59 - ETA: 43s - loss: 0.0176 - acc: 0.59 - ETA: 43s - loss: 0.0176 - acc: 0.59 - ETA: 43s - loss: 0.0176 - acc: 0.59 - ETA: 43s - loss: 0.0176 - acc: 0.59 - ETA: 43s - loss: 0.0176 - acc: 0.59 - ETA: 43s - loss: 0.0176 - acc: 0.59 - ETA: 43s - loss: 0.0176 - acc: 0.59 - ETA: 43s - loss: 0.0175 - acc: 0.59 - ETA: 43s - loss: 0.0175 - acc: 0.59 - ETA: 42s - loss: 0.0175 - acc: 0.59 - ETA: 42s - loss: 0.0175 - acc: 0.59 - ETA: 42s - loss: 0.0175 - acc: 0.59 - ETA: 42s - loss: 0.0175 - acc: 0.59 - ETA: 42s - loss: 0.0175 - acc: 0.59 - ETA: 42s - loss: 0.0174 - acc: 0.59 - ETA: 42s - loss: 0.0174 - acc: 0.59 - ETA: 42s - loss: 0.0174 - acc: 0.59 - ETA: 42s - loss: 0.0174 - acc: 0.59 - ETA: 42s - loss: 0.0174 - acc: 0.59 - ETA: 42s - loss: 0.0174 - acc: 0.59 - ETA: 41s - loss: 0.0174 - acc: 0.59 - ETA: 41s - loss: 0.0174 - acc: 0.59 - ETA: 41s - loss: 0.0173 - acc: 0.59 - ETA: 41s - loss: 0.0173 - acc: 0.59 - ETA: 41s - loss: 0.0173 - acc: 0.59 - ETA: 41s - loss: 0.0173 - acc: 0.59 - ETA: 41s - loss: 0.0173 - acc: 0.59 - ETA: 41s - loss: 0.0173 - acc: 0.59 - ETA: 41s - loss: 0.0173 - acc: 0.59 - ETA: 41s - loss: 0.0172 - acc: 0.59 - ETA: 40s - loss: 0.0172 - acc: 0.59 - ETA: 40s - loss: 0.0172 - acc: 0.59 - ETA: 40s - loss: 0.0172 - acc: 0.59 - ETA: 40s - loss: 0.0172 - acc: 0.59 - ETA: 40s - loss: 0.0172 - acc: 0.59 - ETA: 40s - loss: 0.0172 - acc: 0.59 - ETA: 40s - loss: 0.0172 - acc: 0.59 - ETA: 40s - loss: 0.0172 - acc: 0.59 - ETA: 40s - loss: 0.0172 - acc: 0.59 - ETA: 40s - loss: 0.0171 - acc: 0.59 - ETA: 40s - loss: 0.0171 - acc: 0.59 - ETA: 39s - loss: 0.0171 - acc: 0.59 - ETA: 39s - loss: 0.0171 - acc: 0.59 - ETA: 39s - loss: 0.0171 - acc: 0.59 - ETA: 39s - loss: 0.0171 - acc: 0.59 - ETA: 39s - loss: 0.0171 - acc: 0.59 - ETA: 39s - loss: 0.0171 - acc: 0.59 - ETA: 39s - loss: 0.0171 - acc: 0.59 - ETA: 39s - loss: 0.0170 - acc: 0.59 - ETA: 39s - loss: 0.0170 - acc: 0.59 - ETA: 39s - loss: 0.0170 - acc: 0.59 - ETA: 38s - loss: 0.0170 - acc: 0.59 - ETA: 38s - loss: 0.0170 - acc: 0.59 - ETA: 38s - loss: 0.0170 - acc: 0.59 - ETA: 38s - loss: 0.0170 - acc: 0.59 - ETA: 38s - loss: 0.0170 - acc: 0.59 - ETA: 38s - loss: 0.0169 - acc: 0.59 - ETA: 38s - loss: 0.0169 - acc: 0.59 - ETA: 38s - loss: 0.0169 - acc: 0.59 - ETA: 38s - loss: 0.0169 - acc: 0.59 - ETA: 37s - loss: 0.0169 - acc: 0.59 - ETA: 37s - loss: 0.0169 - acc: 0.59 - ETA: 37s - loss: 0.0169 - acc: 0.59 - ETA: 37s - loss: 0.0169 - acc: 0.59 - ETA: 37s - loss: 0.0169 - acc: 0.59 - ETA: 37s - loss: 0.0168 - acc: 0.59 - ETA: 37s - loss: 0.0168 - acc: 0.59 - ETA: 37s - loss: 0.0168 - acc: 0.59 - ETA: 37s - loss: 0.0168 - acc: 0.59 - ETA: 36s - loss: 0.0168 - acc: 0.59 - ETA: 36s - loss: 0.0168 - acc: 0.59 - ETA: 36s - loss: 0.0168 - acc: 0.59 - ETA: 36s - loss: 0.0168 - acc: 0.59 - ETA: 36s - loss: 0.0168 - acc: 0.59 - ETA: 36s - loss: 0.0167 - acc: 0.59 - ETA: 36s - loss: 0.0167 - acc: 0.59 - ETA: 36s - loss: 0.0167 - acc: 0.59 - ETA: 36s - loss: 0.0167 - acc: 0.59 - ETA: 36s - loss: 0.0167 - acc: 0.59 - ETA: 35s - loss: 0.0167 - acc: 0.59 - ETA: 35s - loss: 0.0167 - acc: 0.59 - ETA: 35s - loss: 0.0167 - acc: 0.59 - ETA: 35s - loss: 0.0167 - acc: 0.59 - ETA: 35s - loss: 0.0167 - acc: 0.59 - ETA: 35s - loss: 0.0166 - acc: 0.59 - ETA: 35s - loss: 0.0166 - acc: 0.59 - ETA: 35s - loss: 0.0166 - acc: 0.59 - ETA: 35s - loss: 0.0166 - acc: 0.59 - ETA: 35s - loss: 0.0166 - acc: 0.59 - ETA: 34s - loss: 0.0166 - acc: 0.59 - ETA: 34s - loss: 0.0166 - acc: 0.59 - ETA: 34s - loss: 0.0166 - acc: 0.59 - ETA: 34s - loss: 0.0166 - acc: 0.59 - ETA: 34s - loss: 0.0165 - acc: 0.59 - ETA: 34s - loss: 0.0165 - acc: 0.59 - ETA: 34s - loss: 0.0165 - acc: 0.59 - ETA: 34s - loss: 0.0165 - acc: 0.59 - ETA: 34s - loss: 0.0165 - acc: 0.5984"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40704/47959 [========================>.....] - ETA: 33s - loss: 0.0165 - acc: 0.59 - ETA: 33s - loss: 0.0165 - acc: 0.59 - ETA: 33s - loss: 0.0165 - acc: 0.59 - ETA: 33s - loss: 0.0165 - acc: 0.59 - ETA: 33s - loss: 0.0165 - acc: 0.59 - ETA: 33s - loss: 0.0165 - acc: 0.59 - ETA: 33s - loss: 0.0164 - acc: 0.59 - ETA: 33s - loss: 0.0164 - acc: 0.59 - ETA: 33s - loss: 0.0164 - acc: 0.59 - ETA: 33s - loss: 0.0164 - acc: 0.59 - ETA: 32s - loss: 0.0164 - acc: 0.59 - ETA: 32s - loss: 0.0164 - acc: 0.59 - ETA: 32s - loss: 0.0164 - acc: 0.59 - ETA: 32s - loss: 0.0164 - acc: 0.59 - ETA: 32s - loss: 0.0164 - acc: 0.59 - ETA: 32s - loss: 0.0164 - acc: 0.59 - ETA: 32s - loss: 0.0164 - acc: 0.59 - ETA: 32s - loss: 0.0164 - acc: 0.59 - ETA: 32s - loss: 0.0163 - acc: 0.59 - ETA: 31s - loss: 0.0163 - acc: 0.59 - ETA: 31s - loss: 0.0163 - acc: 0.59 - ETA: 31s - loss: 0.0163 - acc: 0.59 - ETA: 31s - loss: 0.0163 - acc: 0.59 - ETA: 31s - loss: 0.0163 - acc: 0.59 - ETA: 31s - loss: 0.0163 - acc: 0.59 - ETA: 31s - loss: 0.0163 - acc: 0.59 - ETA: 31s - loss: 0.0163 - acc: 0.59 - ETA: 31s - loss: 0.0163 - acc: 0.59 - ETA: 30s - loss: 0.0162 - acc: 0.59 - ETA: 30s - loss: 0.0162 - acc: 0.59 - ETA: 30s - loss: 0.0162 - acc: 0.59 - ETA: 30s - loss: 0.0162 - acc: 0.59 - ETA: 30s - loss: 0.0162 - acc: 0.59 - ETA: 30s - loss: 0.0162 - acc: 0.59 - ETA: 30s - loss: 0.0162 - acc: 0.59 - ETA: 30s - loss: 0.0162 - acc: 0.59 - ETA: 30s - loss: 0.0162 - acc: 0.59 - ETA: 30s - loss: 0.0162 - acc: 0.59 - ETA: 29s - loss: 0.0162 - acc: 0.59 - ETA: 29s - loss: 0.0162 - acc: 0.59 - ETA: 29s - loss: 0.0161 - acc: 0.59 - ETA: 29s - loss: 0.0161 - acc: 0.59 - ETA: 29s - loss: 0.0161 - acc: 0.59 - ETA: 29s - loss: 0.0161 - acc: 0.59 - ETA: 29s - loss: 0.0161 - acc: 0.59 - ETA: 29s - loss: 0.0161 - acc: 0.59 - ETA: 29s - loss: 0.0161 - acc: 0.59 - ETA: 28s - loss: 0.0161 - acc: 0.59 - ETA: 28s - loss: 0.0161 - acc: 0.59 - ETA: 28s - loss: 0.0161 - acc: 0.59 - ETA: 28s - loss: 0.0161 - acc: 0.60 - ETA: 28s - loss: 0.0161 - acc: 0.60 - ETA: 28s - loss: 0.0161 - acc: 0.60 - ETA: 28s - loss: 0.0161 - acc: 0.60 - ETA: 28s - loss: 0.0160 - acc: 0.60 - ETA: 28s - loss: 0.0160 - acc: 0.60 - ETA: 28s - loss: 0.0160 - acc: 0.60 - ETA: 27s - loss: 0.0160 - acc: 0.60 - ETA: 27s - loss: 0.0160 - acc: 0.60 - ETA: 27s - loss: 0.0160 - acc: 0.60 - ETA: 27s - loss: 0.0160 - acc: 0.60 - ETA: 27s - loss: 0.0160 - acc: 0.60 - ETA: 27s - loss: 0.0160 - acc: 0.60 - ETA: 27s - loss: 0.0160 - acc: 0.60 - ETA: 27s - loss: 0.0160 - acc: 0.60 - ETA: 27s - loss: 0.0159 - acc: 0.60 - ETA: 26s - loss: 0.0159 - acc: 0.60 - ETA: 26s - loss: 0.0159 - acc: 0.60 - ETA: 26s - loss: 0.0159 - acc: 0.60 - ETA: 26s - loss: 0.0159 - acc: 0.60 - ETA: 26s - loss: 0.0159 - acc: 0.60 - ETA: 26s - loss: 0.0159 - acc: 0.60 - ETA: 26s - loss: 0.0159 - acc: 0.60 - ETA: 26s - loss: 0.0159 - acc: 0.60 - ETA: 26s - loss: 0.0159 - acc: 0.60 - ETA: 26s - loss: 0.0159 - acc: 0.60 - ETA: 25s - loss: 0.0159 - acc: 0.60 - ETA: 25s - loss: 0.0159 - acc: 0.60 - ETA: 25s - loss: 0.0159 - acc: 0.60 - ETA: 25s - loss: 0.0158 - acc: 0.60 - ETA: 25s - loss: 0.0158 - acc: 0.60 - ETA: 25s - loss: 0.0158 - acc: 0.60 - ETA: 25s - loss: 0.0158 - acc: 0.60 - ETA: 25s - loss: 0.0158 - acc: 0.60 - ETA: 25s - loss: 0.0158 - acc: 0.60 - ETA: 25s - loss: 0.0158 - acc: 0.60 - ETA: 24s - loss: 0.0158 - acc: 0.60 - ETA: 24s - loss: 0.0158 - acc: 0.60 - ETA: 24s - loss: 0.0158 - acc: 0.60 - ETA: 24s - loss: 0.0158 - acc: 0.60 - ETA: 24s - loss: 0.0158 - acc: 0.60 - ETA: 24s - loss: 0.0158 - acc: 0.60 - ETA: 24s - loss: 0.0158 - acc: 0.60 - ETA: 24s - loss: 0.0157 - acc: 0.60 - ETA: 24s - loss: 0.0157 - acc: 0.60 - ETA: 23s - loss: 0.0157 - acc: 0.60 - ETA: 23s - loss: 0.0157 - acc: 0.60 - ETA: 23s - loss: 0.0157 - acc: 0.60 - ETA: 23s - loss: 0.0157 - acc: 0.60 - ETA: 23s - loss: 0.0157 - acc: 0.60 - ETA: 23s - loss: 0.0157 - acc: 0.60 - ETA: 23s - loss: 0.0157 - acc: 0.60 - ETA: 23s - loss: 0.0157 - acc: 0.60 - ETA: 23s - loss: 0.0157 - acc: 0.60 - ETA: 23s - loss: 0.0157 - acc: 0.60 - ETA: 22s - loss: 0.0157 - acc: 0.60 - ETA: 22s - loss: 0.0157 - acc: 0.60 - ETA: 22s - loss: 0.0156 - acc: 0.60 - ETA: 22s - loss: 0.0156 - acc: 0.60 - ETA: 22s - loss: 0.0156 - acc: 0.60 - ETA: 22s - loss: 0.0156 - acc: 0.60 - ETA: 22s - loss: 0.0156 - acc: 0.60 - ETA: 22s - loss: 0.0156 - acc: 0.60 - ETA: 22s - loss: 0.0156 - acc: 0.60 - ETA: 21s - loss: 0.0156 - acc: 0.60 - ETA: 21s - loss: 0.0156 - acc: 0.60 - ETA: 21s - loss: 0.0156 - acc: 0.60 - ETA: 21s - loss: 0.0156 - acc: 0.60 - ETA: 21s - loss: 0.0156 - acc: 0.60 - ETA: 21s - loss: 0.0156 - acc: 0.60 - ETA: 21s - loss: 0.0156 - acc: 0.60 - ETA: 21s - loss: 0.0155 - acc: 0.60 - ETA: 21s - loss: 0.0155 - acc: 0.60 - ETA: 21s - loss: 0.0155 - acc: 0.60 - ETA: 20s - loss: 0.0155 - acc: 0.60 - ETA: 20s - loss: 0.0155 - acc: 0.61 - ETA: 20s - loss: 0.0155 - acc: 0.61 - ETA: 20s - loss: 0.0155 - acc: 0.61 - ETA: 20s - loss: 0.0155 - acc: 0.61 - ETA: 20s - loss: 0.0155 - acc: 0.61 - ETA: 20s - loss: 0.0155 - acc: 0.61 - ETA: 20s - loss: 0.0155 - acc: 0.61 - ETA: 20s - loss: 0.0155 - acc: 0.61 - ETA: 19s - loss: 0.0155 - acc: 0.61 - ETA: 19s - loss: 0.0155 - acc: 0.61 - ETA: 19s - loss: 0.0155 - acc: 0.61 - ETA: 19s - loss: 0.0154 - acc: 0.61 - ETA: 19s - loss: 0.0154 - acc: 0.61 - ETA: 19s - loss: 0.0154 - acc: 0.61 - ETA: 19s - loss: 0.0154 - acc: 0.61 - ETA: 19s - loss: 0.0154 - acc: 0.61 - ETA: 19s - loss: 0.0154 - acc: 0.61 - ETA: 19s - loss: 0.0154 - acc: 0.61 - ETA: 18s - loss: 0.0154 - acc: 0.61 - ETA: 18s - loss: 0.0154 - acc: 0.61 - ETA: 18s - loss: 0.0154 - acc: 0.61 - ETA: 18s - loss: 0.0154 - acc: 0.61 - ETA: 18s - loss: 0.0154 - acc: 0.61 - ETA: 18s - loss: 0.0154 - acc: 0.61 - ETA: 18s - loss: 0.0154 - acc: 0.61 - ETA: 18s - loss: 0.0154 - acc: 0.61 - ETA: 18s - loss: 0.0154 - acc: 0.61 - ETA: 17s - loss: 0.0154 - acc: 0.61 - ETA: 17s - loss: 0.0154 - acc: 0.61 - ETA: 17s - loss: 0.0153 - acc: 0.61 - ETA: 17s - loss: 0.0153 - acc: 0.61 - ETA: 17s - loss: 0.0153 - acc: 0.61 - ETA: 17s - loss: 0.0153 - acc: 0.61 - ETA: 17s - loss: 0.0153 - acc: 0.61 - ETA: 17s - loss: 0.0153 - acc: 0.61 - ETA: 17s - loss: 0.0153 - acc: 0.61 - ETA: 17s - loss: 0.0153 - acc: 0.61 - ETA: 16s - loss: 0.0153 - acc: 0.61 - ETA: 16s - loss: 0.0153 - acc: 0.61 - ETA: 16s - loss: 0.0153 - acc: 0.61 - ETA: 16s - loss: 0.0153 - acc: 0.61 - ETA: 16s - loss: 0.0153 - acc: 0.61 - ETA: 16s - loss: 0.0153 - acc: 0.61 - ETA: 16s - loss: 0.0153 - acc: 0.61 - ETA: 16s - loss: 0.0153 - acc: 0.61 - ETA: 16s - loss: 0.0152 - acc: 0.61 - ETA: 16s - loss: 0.0152 - acc: 0.61 - ETA: 15s - loss: 0.0152 - acc: 0.61 - ETA: 15s - loss: 0.0152 - acc: 0.61 - ETA: 15s - loss: 0.0152 - acc: 0.61 - ETA: 15s - loss: 0.0152 - acc: 0.61 - ETA: 15s - loss: 0.0152 - acc: 0.61 - ETA: 15s - loss: 0.0152 - acc: 0.61 - ETA: 15s - loss: 0.0152 - acc: 0.61 - ETA: 15s - loss: 0.0152 - acc: 0.61 - ETA: 15s - loss: 0.0152 - acc: 0.61 - ETA: 14s - loss: 0.0152 - acc: 0.61 - ETA: 14s - loss: 0.0152 - acc: 0.61 - ETA: 14s - loss: 0.0152 - acc: 0.61 - ETA: 14s - loss: 0.0152 - acc: 0.61 - ETA: 14s - loss: 0.0152 - acc: 0.61 - ETA: 14s - loss: 0.0152 - acc: 0.61 - ETA: 14s - loss: 0.0152 - acc: 0.61 - ETA: 14s - loss: 0.0151 - acc: 0.61 - ETA: 14s - loss: 0.0151 - acc: 0.61 - ETA: 14s - loss: 0.0151 - acc: 0.61 - ETA: 13s - loss: 0.0151 - acc: 0.61 - ETA: 13s - loss: 0.0151 - acc: 0.61 - ETA: 13s - loss: 0.0151 - acc: 0.61 - ETA: 13s - loss: 0.0151 - acc: 0.61 - ETA: 13s - loss: 0.0151 - acc: 0.61 - ETA: 13s - loss: 0.0151 - acc: 0.61 - ETA: 13s - loss: 0.0151 - acc: 0.61 - ETA: 13s - loss: 0.0151 - acc: 0.61 - ETA: 13s - loss: 0.0151 - acc: 0.61 - ETA: 13s - loss: 0.0151 - acc: 0.61 - ETA: 12s - loss: 0.0151 - acc: 0.61 - ETA: 12s - loss: 0.0151 - acc: 0.61 - ETA: 12s - loss: 0.0151 - acc: 0.61 - ETA: 12s - loss: 0.0151 - acc: 0.61 - ETA: 12s - loss: 0.0151 - acc: 0.61 - ETA: 12s - loss: 0.0151 - acc: 0.61 - ETA: 12s - loss: 0.0151 - acc: 0.61 - ETA: 12s - loss: 0.0150 - acc: 0.61 - ETA: 12s - loss: 0.0150 - acc: 0.61 - ETA: 11s - loss: 0.0150 - acc: 0.61 - ETA: 11s - loss: 0.0150 - acc: 0.61 - ETA: 11s - loss: 0.0150 - acc: 0.61 - ETA: 11s - loss: 0.0150 - acc: 0.61 - ETA: 11s - loss: 0.0150 - acc: 0.6145"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47959/47959 [==============================] - ETA: 11s - loss: 0.0150 - acc: 0.61 - ETA: 11s - loss: 0.0150 - acc: 0.61 - ETA: 11s - loss: 0.0150 - acc: 0.61 - ETA: 11s - loss: 0.0150 - acc: 0.61 - ETA: 11s - loss: 0.0150 - acc: 0.61 - ETA: 10s - loss: 0.0150 - acc: 0.61 - ETA: 10s - loss: 0.0150 - acc: 0.61 - ETA: 10s - loss: 0.0150 - acc: 0.61 - ETA: 10s - loss: 0.0150 - acc: 0.61 - ETA: 10s - loss: 0.0150 - acc: 0.61 - ETA: 10s - loss: 0.0150 - acc: 0.61 - ETA: 10s - loss: 0.0150 - acc: 0.61 - ETA: 10s - loss: 0.0150 - acc: 0.61 - ETA: 10s - loss: 0.0149 - acc: 0.61 - ETA: 10s - loss: 0.0149 - acc: 0.61 - ETA: 9s - loss: 0.0149 - acc: 0.6141 - ETA: 9s - loss: 0.0149 - acc: 0.614 - ETA: 9s - loss: 0.0149 - acc: 0.614 - ETA: 9s - loss: 0.0149 - acc: 0.614 - ETA: 9s - loss: 0.0149 - acc: 0.614 - ETA: 9s - loss: 0.0149 - acc: 0.614 - ETA: 9s - loss: 0.0149 - acc: 0.614 - ETA: 9s - loss: 0.0149 - acc: 0.614 - ETA: 9s - loss: 0.0149 - acc: 0.614 - ETA: 9s - loss: 0.0149 - acc: 0.614 - ETA: 8s - loss: 0.0149 - acc: 0.615 - ETA: 8s - loss: 0.0149 - acc: 0.615 - ETA: 8s - loss: 0.0149 - acc: 0.615 - ETA: 8s - loss: 0.0149 - acc: 0.615 - ETA: 8s - loss: 0.0149 - acc: 0.615 - ETA: 8s - loss: 0.0149 - acc: 0.615 - ETA: 8s - loss: 0.0149 - acc: 0.615 - ETA: 8s - loss: 0.0149 - acc: 0.616 - ETA: 8s - loss: 0.0149 - acc: 0.616 - ETA: 7s - loss: 0.0148 - acc: 0.616 - ETA: 7s - loss: 0.0148 - acc: 0.616 - ETA: 7s - loss: 0.0148 - acc: 0.616 - ETA: 7s - loss: 0.0148 - acc: 0.616 - ETA: 7s - loss: 0.0148 - acc: 0.616 - ETA: 7s - loss: 0.0148 - acc: 0.616 - ETA: 7s - loss: 0.0148 - acc: 0.616 - ETA: 7s - loss: 0.0148 - acc: 0.616 - ETA: 7s - loss: 0.0148 - acc: 0.616 - ETA: 7s - loss: 0.0148 - acc: 0.616 - ETA: 6s - loss: 0.0148 - acc: 0.616 - ETA: 6s - loss: 0.0148 - acc: 0.616 - ETA: 6s - loss: 0.0148 - acc: 0.616 - ETA: 6s - loss: 0.0148 - acc: 0.616 - ETA: 6s - loss: 0.0148 - acc: 0.616 - ETA: 6s - loss: 0.0148 - acc: 0.616 - ETA: 6s - loss: 0.0148 - acc: 0.616 - ETA: 6s - loss: 0.0148 - acc: 0.616 - ETA: 6s - loss: 0.0148 - acc: 0.616 - ETA: 6s - loss: 0.0148 - acc: 0.616 - ETA: 5s - loss: 0.0148 - acc: 0.616 - ETA: 5s - loss: 0.0148 - acc: 0.616 - ETA: 5s - loss: 0.0147 - acc: 0.616 - ETA: 5s - loss: 0.0147 - acc: 0.616 - ETA: 5s - loss: 0.0147 - acc: 0.616 - ETA: 5s - loss: 0.0147 - acc: 0.616 - ETA: 5s - loss: 0.0147 - acc: 0.615 - ETA: 5s - loss: 0.0147 - acc: 0.615 - ETA: 5s - loss: 0.0147 - acc: 0.615 - ETA: 5s - loss: 0.0147 - acc: 0.615 - ETA: 4s - loss: 0.0147 - acc: 0.615 - ETA: 4s - loss: 0.0147 - acc: 0.615 - ETA: 4s - loss: 0.0147 - acc: 0.615 - ETA: 4s - loss: 0.0147 - acc: 0.615 - ETA: 4s - loss: 0.0147 - acc: 0.615 - ETA: 4s - loss: 0.0147 - acc: 0.615 - ETA: 4s - loss: 0.0147 - acc: 0.615 - ETA: 4s - loss: 0.0147 - acc: 0.616 - ETA: 4s - loss: 0.0147 - acc: 0.616 - ETA: 3s - loss: 0.0147 - acc: 0.616 - ETA: 3s - loss: 0.0147 - acc: 0.616 - ETA: 3s - loss: 0.0147 - acc: 0.616 - ETA: 3s - loss: 0.0147 - acc: 0.616 - ETA: 3s - loss: 0.0147 - acc: 0.616 - ETA: 3s - loss: 0.0147 - acc: 0.616 - ETA: 3s - loss: 0.0147 - acc: 0.616 - ETA: 3s - loss: 0.0146 - acc: 0.616 - ETA: 3s - loss: 0.0146 - acc: 0.616 - ETA: 3s - loss: 0.0146 - acc: 0.616 - ETA: 2s - loss: 0.0146 - acc: 0.616 - ETA: 2s - loss: 0.0146 - acc: 0.616 - ETA: 2s - loss: 0.0146 - acc: 0.616 - ETA: 2s - loss: 0.0146 - acc: 0.616 - ETA: 2s - loss: 0.0146 - acc: 0.616 - ETA: 2s - loss: 0.0146 - acc: 0.616 - ETA: 2s - loss: 0.0146 - acc: 0.616 - ETA: 2s - loss: 0.0146 - acc: 0.616 - ETA: 2s - loss: 0.0146 - acc: 0.616 - ETA: 2s - loss: 0.0146 - acc: 0.616 - ETA: 1s - loss: 0.0146 - acc: 0.616 - ETA: 1s - loss: 0.0146 - acc: 0.616 - ETA: 1s - loss: 0.0146 - acc: 0.616 - ETA: 1s - loss: 0.0146 - acc: 0.616 - ETA: 1s - loss: 0.0146 - acc: 0.616 - ETA: 1s - loss: 0.0146 - acc: 0.616 - ETA: 1s - loss: 0.0146 - acc: 0.616 - ETA: 1s - loss: 0.0146 - acc: 0.616 - ETA: 1s - loss: 0.0146 - acc: 0.616 - ETA: 1s - loss: 0.0146 - acc: 0.616 - ETA: 0s - loss: 0.0146 - acc: 0.616 - ETA: 0s - loss: 0.0145 - acc: 0.616 - ETA: 0s - loss: 0.0145 - acc: 0.617 - ETA: 0s - loss: 0.0145 - acc: 0.617 - ETA: 0s - loss: 0.0145 - acc: 0.617 - ETA: 0s - loss: 0.0145 - acc: 0.617 - ETA: 0s - loss: 0.0145 - acc: 0.617 - ETA: 0s - loss: 0.0145 - acc: 0.617 - ETA: 0s - loss: 0.0145 - acc: 0.617 - ETA: 0s - loss: 0.0145 - acc: 0.618 - 76s 2ms/sample - loss: 0.0145 - acc: 0.6181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bc359609b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.fit(process.train,process.labels, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47959, 30, 9)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_8 (Bidirection multiple                  7632      \n",
      "=================================================================\n",
      "Total params: 7,632\n",
      "Trainable params: 7,632\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(process.train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Process\n",
    "from model import MyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1bd185f3f28>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.48749703,  1.4675021 , -0.16871226, -2.3522685 , -0.33267498,\n",
       "       -1.3031332 , -1.7944415 , -0.8939359 , -3.490642  , -0.702857  ,\n",
       "        4.8056912 ,  2.7493382 , -1.7890092 , -2.9456358 , -2.9584503 ,\n",
       "       -0.40190887,  0.37479514,  0.8251643 , -0.16274163,  1.3356256 ,\n",
       "       -0.60285527,  2.946636  ,  0.0660156 , -2.7387836 , -0.9132359 ,\n",
       "        1.2184515 , -1.6043463 ,  4.6903625 ,  2.0782776 ,  2.9761586 ,\n",
       "       -0.47113055,  0.3363287 , -1.8168869 ,  2.1025822 , -2.639567  ,\n",
       "       -0.50544095,  4.25952   , -3.0765858 , -0.9500612 , -1.8106607 ,\n",
       "       -4.680292  ,  3.9034457 , -0.4931122 , -2.3818793 ,  0.7727964 ,\n",
       "        1.2954992 , -0.83070743, -1.5728328 , -3.389197  ,  0.63852525,\n",
       "       -1.6846281 ,  0.28218997,  3.0872025 , -1.5166373 , -2.4015412 ,\n",
       "       -2.5796614 ,  3.0967555 ,  0.86297596, -3.378342  ,  2.1407802 ,\n",
       "        3.8458    ,  1.6987417 , -1.3002472 , -1.2903582 ,  0.8801274 ,\n",
       "        1.2779676 ,  2.2427807 ,  0.6119989 ,  1.8200396 ,  0.63721144,\n",
       "       -1.6819801 , -0.2091321 , -3.1487858 , -3.9448879 ,  0.96679676,\n",
       "       -1.0797068 , -1.1823401 ,  1.0571418 ,  0.18595102,  0.01061344,\n",
       "        1.1149918 ,  2.0634317 , -2.1243243 ,  1.3031912 ,  2.4950666 ,\n",
       "       -0.58870506,  1.4748033 , -0.50489837,  6.0643334 , -0.38882467,\n",
       "        1.1689074 ,  5.136613  , -1.2224591 , -1.8154638 , -0.553221  ,\n",
       "        2.2296872 ], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('hi').vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=(process.text_prep('I am in London tomorrow I will go to new york'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "process=Process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0703 17:13:51.411604 23636 deprecation.py:506] From d:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0703 17:13:51.417941 23636 deprecation.py:506] From d:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0703 17:13:51.419934 23636 deprecation.py:506] From d:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0703 17:13:51.421926 23636 deprecation.py:506] From d:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0703 17:13:51.929153 23636 hdf5_format.py:263] Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x1ef7921b278>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "transpose expects a vector of size 2. But input(1) is a vector of size 3\n\t [[{{node sequential/bidirectional/transpose_2}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-8c4c6395f80e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Projects\\Machine Learning\\ner-dl\\model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, predict)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1076\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m           \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1078\u001b[1;33m           callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1079\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: transpose expects a vector of size 2. But input(1) is a vector of size 3\n\t [[{{node sequential/bidirectional/transpose_2}}]]"
     ]
    }
   ],
   "source": [
    "model.predict(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "text_prep() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-081d63c350f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_prep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'You may have noticed in several Keras recurrent layers there are two'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'parameters return state and return sequences In this post I am going to show you what they mean and when to use them in real-life cases'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: text_prep() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "a=(process.text_prep('You may have noticed in several Keras recurrent layers there are two','parameters return state and return sequences In this post I am going to show you what they mean and when to use them in real-life cases'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37807152, -4.5718184 ,  0.75758845, -0.62460047, -3.7791712 ,\n",
       "         0.517763  ,  0.1624111 , -0.7977886 ,  1.4929111 ,  1.973931  ,\n",
       "         0.693751  ,  3.2226062 , -3.8530154 , -0.5382964 , -2.0138326 ,\n",
       "        -0.38041484,  0.4861796 , -1.5869466 ,  3.3632572 ,  0.2952693 ,\n",
       "         1.6519415 ,  0.38939497,  2.1507425 , -0.8321895 , -0.46887755,\n",
       "         3.8721411 ,  3.8127131 , -1.9056518 ,  2.5369124 , -0.14575773],\n",
       "       [ 0.01091194,  1.8029234 , -0.8110889 ,  0.05973989, -1.7836052 ,\n",
       "        -2.6238225 , -2.3235142 , -2.036842  ,  1.245089  ,  3.0557609 ,\n",
       "        -2.2395184 , -0.15557945, -1.8089032 , -0.20564681,  5.362402  ,\n",
       "         0.7999551 ,  0.29076514, -1.938662  ,  0.69367075, -2.7016225 ,\n",
       "         0.10439445, -1.0764995 ,  4.9380307 ,  0.7993517 , -0.53913283,\n",
       "         2.8357725 , -0.67980653, -1.8791453 , -3.0485466 ,  3.5317063 ],\n",
       "       [-0.45486706, -0.42158654, -0.5655305 , -0.68311816, -2.343575  ,\n",
       "         0.75955486, -1.5440092 , -2.1316593 ,  2.7622461 ,  1.4967352 ,\n",
       "        -1.8550059 , -1.3378587 ,  0.02343833, -2.1009943 ,  2.728036  ,\n",
       "        -1.151348  , -1.8207653 , -1.2855977 ,  2.376491  ,  1.1934183 ,\n",
       "         0.09857939,  0.53031796,  1.7395838 ,  7.567287  ,  2.71313   ,\n",
       "         8.091698  , -0.12723836,  0.1880781 , -3.0979593 ,  0.03698458],\n",
       "       [ 2.4636059 , -1.0097833 ,  0.01207095,  1.7918532 , -1.146946  ,\n",
       "        -1.755069  , -2.5920773 ,  0.46495706,  2.4784958 ,  0.68700224,\n",
       "        -1.3918419 ,  2.6622503 , -3.6958926 ,  1.0723163 , -2.1600986 ,\n",
       "         1.688318  , -0.71191543,  4.3192444 , -0.4935341 , -2.5672848 ,\n",
       "         1.6829278 ,  2.3609123 ,  4.440076  , -0.9492338 ,  2.442618  ,\n",
       "         0.17147678, -1.2479749 , -1.4729673 ,  1.9205573 ,  1.4377476 ],\n",
       "       [-1.0115339 ,  0.26481897, -2.7919514 ,  5.6706643 , -0.20847356,\n",
       "        -1.28041   ,  2.4751825 ,  3.7185063 ,  2.138344  ,  3.781839  ,\n",
       "        -0.6770344 ,  1.2469934 , -3.9677882 ,  0.29198337, -1.5842916 ,\n",
       "         0.1978631 , -4.2988734 ,  3.00767   ,  0.01126885, -1.7318648 ,\n",
       "         0.68321574, -1.7543099 ,  5.3752117 , -2.5538473 ,  0.3169942 ,\n",
       "        -1.8183602 ,  1.7357477 , -2.8359084 ,  0.08015561,  4.6539273 ],\n",
       "       [ 0.37807152, -4.5718184 ,  0.75758845, -0.62460047, -3.7791712 ,\n",
       "         0.517763  ,  0.1624111 , -0.7977886 ,  1.4929111 ,  1.973931  ,\n",
       "         0.693751  ,  3.2226062 , -3.8530154 , -0.5382964 , -2.0138326 ,\n",
       "        -0.38041484,  0.4861796 , -1.5869466 ,  3.3632572 ,  0.2952693 ,\n",
       "         1.6519415 ,  0.38939497,  2.1507425 , -0.8321895 , -0.46887755,\n",
       "         3.8721411 ,  3.8127131 , -1.9056518 ,  2.5369124 , -0.14575773],\n",
       "       [-3.7932808 , -1.4702656 , -2.0384748 ,  1.1289765 , -2.6051295 ,\n",
       "         1.4177219 , -2.238142  ,  0.6226727 ,  0.60316503,  0.5260997 ,\n",
       "        -2.1482952 , -0.85306865, -2.465991  ,  2.0224922 ,  2.1482415 ,\n",
       "         1.6414363 , -2.113275  ,  0.43463838,  2.531138  ,  2.2102566 ,\n",
       "        -0.29190636,  0.26796073,  1.9236808 , -3.7389057 , -0.47154337,\n",
       "         2.0137897 ,  4.974401  , -3.2126555 , -3.0719907 ,  5.7077594 ],\n",
       "       [ 1.7148497 , -0.90720767,  3.830694  ,  0.19920897, -0.28959155,\n",
       "        -2.9385757 , -4.2424645 , -1.4538815 ,  1.101549  , -0.71783054,\n",
       "        -0.1327397 ,  0.04959074, -1.4451337 ,  3.0431345 ,  3.8001323 ,\n",
       "         0.33650386, -2.3112698 ,  0.43462306, -0.4825771 , -2.5645163 ,\n",
       "         0.78265595,  1.6430078 ,  5.3645697 , -0.8322612 ,  0.04026246,\n",
       "         0.6457575 ,  3.2974906 ,  0.9401473 , -1.0578339 ,  1.8883286 ],\n",
       "       [-1.2860768 ,  0.5857779 , -0.9861128 ,  0.3134587 , -3.3353882 ,\n",
       "         1.0031537 , -2.0909462 , -3.6222463 ,  2.2702596 ,  3.9886112 ,\n",
       "         1.0853479 ,  0.62446845, -3.1491432 , -1.0064546 ,  5.0941334 ,\n",
       "        -0.5497086 , -1.2211554 , -2.1522174 ,  2.9957552 ,  0.7010538 ,\n",
       "        -0.5151347 ,  2.1222844 ,  1.6895415 ,  3.4629388 ,  1.0666363 ,\n",
       "         7.533019  ,  2.2152324 , -2.1205618 , -2.4443743 ,  2.5388162 ],\n",
       "       [ 3.4302032 , -1.3084121 , -2.5906847 , -0.064826  , -1.6886336 ,\n",
       "         1.8373071 ,  0.59235567, -0.4390791 ,  4.504613  , -0.8967925 ,\n",
       "        -1.6466126 ,  0.18989116, -1.199035  ,  0.8208091 , -3.1796803 ,\n",
       "         0.4567206 , -2.8456295 ,  0.8899516 ,  3.4488876 , -1.009959  ,\n",
       "         2.380714  ,  2.445496  ,  3.6307645 , -1.8358951 ,  7.163365  ,\n",
       "         1.8112268 , -0.36542404, -2.0286613 ,  3.329437  ,  0.02165848],\n",
       "       [ 2.4015875 ,  0.4222733 , -0.68301445,  3.3770852 ,  1.7404286 ,\n",
       "        -0.04576147,  0.7382008 , -0.48619398, -1.3863573 , -0.47846764,\n",
       "        -0.5938965 ,  2.2199755 , -2.229835  , -2.2103536 , -2.8794315 ,\n",
       "         2.1345427 , -4.5815697 ,  4.2141404 , -1.2455773 , -2.7641299 ,\n",
       "         1.1208397 , -0.71423763,  7.640603  , -0.9487624 ,  3.3716378 ,\n",
       "        -0.0639991 , -2.4462984 , -1.6516373 , -2.344947  ,  2.640092  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = pad_sequences(a, maxlen=30, dtype='float32', padding='pre', truncating='pre', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37807152, -4.5718184 ,  0.75758845, -0.62460047, -3.7791712 ,\n",
       "         0.517763  ,  0.1624111 , -0.7977886 ,  1.4929111 ,  1.973931  ,\n",
       "         0.693751  ,  3.2226062 , -3.8530154 , -0.5382964 , -2.0138326 ,\n",
       "        -0.38041484,  0.4861796 , -1.5869466 ,  3.3632572 ,  0.2952693 ,\n",
       "         1.6519415 ,  0.38939497,  2.1507425 , -0.8321895 , -0.46887755,\n",
       "         3.8721411 ,  3.8127131 , -1.9056518 ,  2.5369124 , -0.14575773],\n",
       "       [ 0.01091194,  1.8029234 , -0.8110889 ,  0.05973989, -1.7836052 ,\n",
       "        -2.6238225 , -2.3235142 , -2.036842  ,  1.245089  ,  3.0557609 ,\n",
       "        -2.2395184 , -0.15557945, -1.8089032 , -0.20564681,  5.362402  ,\n",
       "         0.7999551 ,  0.29076514, -1.938662  ,  0.69367075, -2.7016225 ,\n",
       "         0.10439445, -1.0764995 ,  4.9380307 ,  0.7993517 , -0.53913283,\n",
       "         2.8357725 , -0.67980653, -1.8791453 , -3.0485466 ,  3.5317063 ],\n",
       "       [-0.45486706, -0.42158654, -0.5655305 , -0.68311816, -2.343575  ,\n",
       "         0.75955486, -1.5440092 , -2.1316593 ,  2.7622461 ,  1.4967352 ,\n",
       "        -1.8550059 , -1.3378587 ,  0.02343833, -2.1009943 ,  2.728036  ,\n",
       "        -1.151348  , -1.8207653 , -1.2855977 ,  2.376491  ,  1.1934183 ,\n",
       "         0.09857939,  0.53031796,  1.7395838 ,  7.567287  ,  2.71313   ,\n",
       "         8.091698  , -0.12723836,  0.1880781 , -3.0979593 ,  0.03698458],\n",
       "       [ 2.4636059 , -1.0097833 ,  0.01207095,  1.7918532 , -1.146946  ,\n",
       "        -1.755069  , -2.5920773 ,  0.46495706,  2.4784958 ,  0.68700224,\n",
       "        -1.3918419 ,  2.6622503 , -3.6958926 ,  1.0723163 , -2.1600986 ,\n",
       "         1.688318  , -0.71191543,  4.3192444 , -0.4935341 , -2.5672848 ,\n",
       "         1.6829278 ,  2.3609123 ,  4.440076  , -0.9492338 ,  2.442618  ,\n",
       "         0.17147678, -1.2479749 , -1.4729673 ,  1.9205573 ,  1.4377476 ],\n",
       "       [-1.0115339 ,  0.26481897, -2.7919514 ,  5.6706643 , -0.20847356,\n",
       "        -1.28041   ,  2.4751825 ,  3.7185063 ,  2.138344  ,  3.781839  ,\n",
       "        -0.6770344 ,  1.2469934 , -3.9677882 ,  0.29198337, -1.5842916 ,\n",
       "         0.1978631 , -4.2988734 ,  3.00767   ,  0.01126885, -1.7318648 ,\n",
       "         0.68321574, -1.7543099 ,  5.3752117 , -2.5538473 ,  0.3169942 ,\n",
       "        -1.8183602 ,  1.7357477 , -2.8359084 ,  0.08015561,  4.6539273 ],\n",
       "       [ 0.37807152, -4.5718184 ,  0.75758845, -0.62460047, -3.7791712 ,\n",
       "         0.517763  ,  0.1624111 , -0.7977886 ,  1.4929111 ,  1.973931  ,\n",
       "         0.693751  ,  3.2226062 , -3.8530154 , -0.5382964 , -2.0138326 ,\n",
       "        -0.38041484,  0.4861796 , -1.5869466 ,  3.3632572 ,  0.2952693 ,\n",
       "         1.6519415 ,  0.38939497,  2.1507425 , -0.8321895 , -0.46887755,\n",
       "         3.8721411 ,  3.8127131 , -1.9056518 ,  2.5369124 , -0.14575773],\n",
       "       [-3.7932808 , -1.4702656 , -2.0384748 ,  1.1289765 , -2.6051295 ,\n",
       "         1.4177219 , -2.238142  ,  0.6226727 ,  0.60316503,  0.5260997 ,\n",
       "        -2.1482952 , -0.85306865, -2.465991  ,  2.0224922 ,  2.1482415 ,\n",
       "         1.6414363 , -2.113275  ,  0.43463838,  2.531138  ,  2.2102566 ,\n",
       "        -0.29190636,  0.26796073,  1.9236808 , -3.7389057 , -0.47154337,\n",
       "         2.0137897 ,  4.974401  , -3.2126555 , -3.0719907 ,  5.7077594 ],\n",
       "       [ 1.7148497 , -0.90720767,  3.830694  ,  0.19920897, -0.28959155,\n",
       "        -2.9385757 , -4.2424645 , -1.4538815 ,  1.101549  , -0.71783054,\n",
       "        -0.1327397 ,  0.04959074, -1.4451337 ,  3.0431345 ,  3.8001323 ,\n",
       "         0.33650386, -2.3112698 ,  0.43462306, -0.4825771 , -2.5645163 ,\n",
       "         0.78265595,  1.6430078 ,  5.3645697 , -0.8322612 ,  0.04026246,\n",
       "         0.6457575 ,  3.2974906 ,  0.9401473 , -1.0578339 ,  1.8883286 ],\n",
       "       [-1.2860768 ,  0.5857779 , -0.9861128 ,  0.3134587 , -3.3353882 ,\n",
       "         1.0031537 , -2.0909462 , -3.6222463 ,  2.2702596 ,  3.9886112 ,\n",
       "         1.0853479 ,  0.62446845, -3.1491432 , -1.0064546 ,  5.0941334 ,\n",
       "        -0.5497086 , -1.2211554 , -2.1522174 ,  2.9957552 ,  0.7010538 ,\n",
       "        -0.5151347 ,  2.1222844 ,  1.6895415 ,  3.4629388 ,  1.0666363 ,\n",
       "         7.533019  ,  2.2152324 , -2.1205618 , -2.4443743 ,  2.5388162 ],\n",
       "       [ 3.4302032 , -1.3084121 , -2.5906847 , -0.064826  , -1.6886336 ,\n",
       "         1.8373071 ,  0.59235567, -0.4390791 ,  4.504613  , -0.8967925 ,\n",
       "        -1.6466126 ,  0.18989116, -1.199035  ,  0.8208091 , -3.1796803 ,\n",
       "         0.4567206 , -2.8456295 ,  0.8899516 ,  3.4488876 , -1.009959  ,\n",
       "         2.380714  ,  2.445496  ,  3.6307645 , -1.8358951 ,  7.163365  ,\n",
       "         1.8112268 , -0.36542404, -2.0286613 ,  3.329437  ,  0.02165848],\n",
       "       [ 2.4015875 ,  0.4222733 , -0.68301445,  3.3770852 ,  1.7404286 ,\n",
       "        -0.04576147,  0.7382008 , -0.48619398, -1.3863573 , -0.47846764,\n",
       "        -0.5938965 ,  2.2199755 , -2.229835  , -2.2103536 , -2.8794315 ,\n",
       "         2.1345427 , -4.5815697 ,  4.2141404 , -1.2455773 , -2.7641299 ,\n",
       "         1.1208397 , -0.71423763,  7.640603  , -0.9487624 ,  3.3716378 ,\n",
       "        -0.0639991 , -2.4462984 , -1.6516373 , -2.344947  ,  2.640092  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "a.append(a)\n",
    "a.append(a)\n",
    "a = pad_sequences(a, maxlen=30, dtype='float32', padding='pre', truncating='pre', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0703 17:38:00.534882  8864 deprecation.py:506] From d:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0703 17:38:00.543857  8864 deprecation.py:506] From d:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0703 17:38:00.544864  8864 deprecation.py:506] From d:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0703 17:38:00.545860  8864 deprecation.py:506] From d:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0703 17:38:01.092389  8864 hdf5_format.py:263] Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model=tf.keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prep(line):\n",
    "\n",
    "    line=tf.keras.preprocessing.text.text_to_word_sequence(line, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                                               lower=True, split=' ')\n",
    "    line = [nlp(word).vector for word in line if word.isalpha()]\n",
    "    line = pad_sequences(line, maxlen=30, dtype='float32', padding='pre', truncating='pre', value=0.0)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=['You may have noticed in several Keras recurrent layers there are two','parameters return state and return sequences In this post I am going to show you what they mean and when to use them in real-life cases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You may have noticed in several Keras recurrent layers there are two',\n",
       " 'parameters return state and return sequences In this post I am going to show you what they mean and when to use them in real-life cases']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7ffe92bf0e40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_prep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-b03136d3e0c1>\u001b[0m in \u001b[0;36mtext_prep\u001b[1;34m(line)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     line=tf.keras.preprocessing.text.text_to_word_sequence(line, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n\u001b[1;32m----> 4\u001b[1;33m                                                                lower=True, split=' ')\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pre'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncating\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pre'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\keras_preprocessing\\text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(text, filters, lower, split)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \"\"\"\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "text_prep(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "process=Process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You may have noticed in several Keras recurrent layers there are two',\n",
       " 'parameters return state and return sequences In this post I am going to show you what they mean and when to use them in real-life cases']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.text_prep('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=tf.keras.preprocessing.text.text_to_word_sequence('parameters return state and return sequences In this post I am going to show you what they mean and when to use them in real-life cases', filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                                               lower=True, split=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test=np.zeros((2,30,96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 30, 96)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "a=[nlp(word).vector for word in z ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0,-27:]=a\n",
    "test[1,-27:]=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.7292352 ,  2.530743  ,  1.0035318 , ...,  2.1770978 ,\n",
       "        -1.841098  ,  1.175618  ],\n",
       "       [ 6.2095094 ,  0.22787511,  2.881082  , ..., -1.7398065 ,\n",
       "        -0.38697553,  2.7573118 ],\n",
       "       [ 5.4892616 , -1.6186669 ,  0.03013329, ...,  0.16546988,\n",
       "        -3.2921379 ,  6.843191  ],\n",
       "       ...,\n",
       "       [-1.7494378 ,  0.6855588 , -3.2083502 , ...,  0.3383305 ,\n",
       "         1.5717316 , -1.1931692 ],\n",
       "       [ 4.6285577 , -1.7490666 , -0.38645157, ..., -1.5811195 ,\n",
       "        -2.7300506 ,  5.469577  ],\n",
       "       [ 0.82409424,  2.642294  ,  1.1184231 , ...,  2.522413  ,\n",
       "        -0.12544331, -0.3536613 ]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[-28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-c0ac8462bce6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\python\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1076\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m           \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1078\u001b[1;33m           callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1079\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m   \u001b[1;31m# Get step function and loop type.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m   \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m   \u001b[0muse_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_dataset\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m   \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_inputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[1;34m(model, mode)\u001b[0m\n\u001b[0;32m    530\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m   2280\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2281\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2282\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2283\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_predict_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2270\u001b[0m             \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_updates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'predict_function'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2272\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   2273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2274\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[0;32m   3477\u001b[0m                'backend') % key\n\u001b[0;32m   3478\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3479\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mGraphExecutionFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, updates, name, **session_kwargs)\u001b[0m\n\u001b[0;32m   3140\u001b[0m     \u001b[1;31m# dependencies in call.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3141\u001b[0m     \u001b[1;31m# Index 0 = total loss or model output for `predict`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3142\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3143\u001b[0m       \u001b[0mupdates_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3144\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional multiple                  296400    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  2709      \n",
      "=================================================================\n",
      "Total params: 299,109\n",
      "Trainable params: 299,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
